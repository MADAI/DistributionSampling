#!/usr/bin/env python2

###########################################################################
#
#  Copyright 2011-2013 The University of North Carolina at Chapel Hill
#  and Michigan State University. All rights reserved.
#
#  Licensed under the MADAI Software License. You may obtain a copy of
#  this license at
#
#      https://madai-public.cs.unc.edu/visualization/software-license/
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
###########################################################################

import os
import sys
import copy
import math
import gzip
import base64
import copy
import StringIO
import matplotlib as mpl
import matplotlib.cm as cm
import matplotlib.pyplot as plt
from matplotlib import colorbar
import matplotlib.patheffects as PathEffects
from scipy import ndimage
import numpy as np

#Global settings for now:
N_SIGMA = 4
PIXEL_WIDTH = 2000

class Histogram(object):
    def __init__(self, N_bins, low, high):
        self.bins = np.array([ low + float(i)*(high - low)/float(N_bins) for i in range(N_bins + 1) ])
        self.content = np.array([0.0]*(N_bins+2)) # +2 for overflow
        self.sum, self.sum2, self.count = 0, 0, 0

    def fill(self, x, w = 1.0):
        if not hasattr(x, '__iter__'):
            x = [x]
        self.content[np.digitize(x, self.bins)] += w
        for value in x:
            self.sum += value*w
            self.sum2 += value*value*w
            self.count += w

    def mu(self):
        return self.sum/self.count

    def sigma(self):
        return np.sqrt(self.sum2/self.count - self.mu()**2)

    def img(self, xlabel = '', ylabel = '', mu = None, sigma = None, format = 'png', width = 100, filename = None):
        plt.bar(self.bins[:-1], self.content[1:-1], self.bins[1]-self.bins[0], color=(0.9,0.2,0.4,0.5), edgecolor=(0.9,0.2,0.4,0.9))
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.xlim([self.bins[0],self.bins[-1]])
        x0, x1, y0, y1 = plt.axis()
        if mu and sigma:
            plt.plot([mu-sigma]*2,[y0,y1],ls='dashed',lw=3,label=r'experimental 1 $\sigma$', color=(0.5,0,0.5,0.5))
            plt.plot([mu+sigma]*2,[y0,y1],ls='dashed',lw=3,color=(0.5,0,0.5,0.5))
            plt.plot([mu]*2,[y0,y1],ls='solid',lw=5,color=(0.5,0,0.5,0.8))
            posterior_mu, posterior_sigma = self.mu(), self.sigma()
            plt.plot([posterior_mu-posterior_sigma]*2,[y0,y1],ls='dashed',lw=3,label=r'posterior 1 $\sigma$', color=(0.5,0.5,0,0.7))
            plt.plot([posterior_mu+posterior_sigma]*2,[y0,y1],ls='dashed',lw=3,color=(0.5,0.5,0,0.7))
            plt.plot([posterior_mu]*2,[y0,y1],ls='solid',lw=5,color=(0.5,0.5,0,1))
            plt.legend()
        #plt.set_inches_size(3,2)
        if filename != None:
            plt.savefig(filename)
        imgdata = StringIO.StringIO()
        plt.savefig(imgdata, format=format, dpi=float(width)/3.0)
        plt.clf()
        imgdata.seek(0)

        return imgdata.read()

#finds the z-magnitudes that correspond to the one and two sigma confidence levels
def find_sigmas(content):
    x = np.sort(content.flatten())[::-1]
    x_int = np.cumsum(x)
    total = x_int[-1]
    one = total*0.6826895
    two = total*0.9544997
    one_arg = np.argmin(abs(x_int-one))
    two_arg = np.argmin(abs(x_int-two))
    return x[one_arg], x[two_arg]

class Histogram2D(object):
    def __init__(self, N_binsx, lowx, highx, N_binsy, lowy, highy):
        self.binsx = np.array([ lowx + float(i)*(highx - lowx)/float(N_binsx) for i in range(N_binsx + 1) ])
        self.binsy = np.array([ lowy + float(i)*(highy - lowy)/float(N_binsy) for i in range(N_binsy + 1) ])
        self.content = np.zeros((N_binsy+2,N_binsx+2)) # +2 for overflow
        self.sumx, self.sum2x, self.sumy, self.sum2y, self.count = 0, 0, 0, 0, 0

    def fill(self, x, y, w = 1.0):
        #not supporting arrays for now
        assert(not hasattr(x, '__iter__'))
        assert(not hasattr(y, '__iter__'))

        self.content[np.digitize([y], self.binsy)[0]][np.digitize([x], self.binsx)[0]] += w
        self.sumx += x*w
        self.sum2x += x*x*w
        self.sumy += y*w
        self.sum2y += y*y*w
        self.count += w

    def mu(self):
        return (self.sumx/self.count, self.sumy/self.count)

    def sigma(self):
        mu = self.mu()
        return np.sqrt( (self.sum2x/self.count - mu[0]**2, self.sum2y/self.count - mu[1]**2) )

    def img(self, xlabel = '', ylabel = '', format = 'png', width = 100, filename = None):
        plt.pcolor(self.binsx, self.binsy, self.content[1:-1,1:-1], edgecolor=(1,1,1,0.3))
        #smoothed_content = ndimage.filters.gaussian_filter(self.content[1:-1,1:-1],1.,mode='constant',cval=0)
        #don't smooth it for now:
        smoothed_content = self.content[1:-1,1:-1]
        edged_content = 0.5*(np.hstack((smoothed_content, np.transpose(np.array([smoothed_content[:,-1]])))) \
                           + np.hstack((np.transpose(np.array([smoothed_content[:,0]])), smoothed_content)))
        edged_content = 0.5*(np.vstack((edged_content, np.array([edged_content[-1,:]]))) \
                           + np.vstack((np.array([edged_content[0,:]]), edged_content)))
        sigmas = find_sigmas(edged_content)
        con = plt.contour(self.binsx, self.binsy, edged_content, sigmas, colors='k', linewidths=2, linestyles='solid')
        cl = plt.clabel(con, fmt={ sigmas[0] : r'$1 \sigma$', sigmas[1] : r'$2 \sigma$'}, fontsize=20)
        plt.setp(cl, path_effects=[PathEffects.withStroke(linewidth=2, foreground="w")])
        plt.setp(con.collections, path_effects=[PathEffects.withStroke(linewidth=5, foreground="w")])

        plt.xlabel(xlabel)
        plt.ylabel(ylabel)
        plt.xlim([self.binsx[0],self.binsx[-1]])
        plt.ylim([self.binsy[0],self.binsy[-1]])
        #plt.set_inches_size(3, 2)
        if filename != None:
            plt.savefig(filename)
        imgdata = StringIO.StringIO()
        plt.savefig(imgdata, format=format, dpi=float(width)/3.0)
        plt.clf()
        imgdata.seek(0)

        return imgdata.read()

class Table(object):
    def __init__(self, html_class, rows, columns, row_type, column_type):
        valid_types = ['parameter', 'observable', 'other']
        assert row_type in valid_types
        assert column_type in valid_types

        valid_types = ['data_table', 'image_table']
        assert html_class in valid_types

        self.html_class = html_class
        self.rows = rows
        self.columns = columns
        self.row_type = row_type
        self.column_type = column_type

class DataTable(Table):
    def __init__(self, rows, columns, row_type, column_type, data, cmap = cm.seismic, zero_center = True):
        Table.__init__(self, html_class = 'data_table', rows = rows, columns = columns, \
                       row_type = row_type, column_type = column_type)

        self.cmap = cmap
        self.zero_center = zero_center

        self.data = copy.deepcopy(data)
        self.colors = copy.deepcopy(data)
        self.colorize()

        self.csv = ''
        for i in range(-1,len(rows)):
            self.csv += '"'
            if i >= 0:
                self.csv += rows[i]
            self.csv += '"'

            for j in range(len(columns)):
                if i < 0:
                    self.csv += ',"' + columns[j] + '"'
                else:
                    self.csv += ',' + str(self.data[i][j])
            self.csv += '\n'
        self.csv = 'data:text/plain;base64,\n' + base64.b64encode(self.csv)

    def produce_scalar_map(self, l):
        vmax = max(l)
        vmin = min(l)
        if self.zero_center:
            vmax = max( [abs(vmin), abs(vmax)] )
            vmin = - vmax
        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)
        scalar_map = cm.ScalarMappable(norm=norm, cmap=self.cmap)
        return scalar_map

    def rgb(self, scalar_map, x):
        return 'rgb' + str(tuple(map(lambda x: int(256*x), scalar_map.to_rgba(x))[:3]))

    def colorize(self):
        scalar_map = self.produce_scalar_map(sum(self.data, []))
        for i in range(len(self.rows)):
            for j in range(len(self.columns)):
                self.colors[i][j] = self.rgb(scalar_map, self.data[i][j])

    def colorize_by_row(self):
        for i in range(len(self.rows)):
            scalar_map = self.produce_scalar_map(data[i])
            for j in range(len(self.columns)):
                self.colors[i][j] = self.rgb(scalar_map, self.data[i][j])

    def colorize_by_column(self):
        for j in range(len(self.columns)):
            scalar_map = self.produce_scalar_map([self.data[i][j] for i in range(len(self.rows))])
            for i in range(len(self.rows)):
                self.colors[i][j] = self.rgb(scalar_map, self.data[i][j])

    def white_out(self):
        for i in range(len(self.rows)):
            for j in range(len(self.columns)):
                self.colors[i][j] = 'rgb(255,255,255)'

class ImageTable(Table):
    def __init__(self, rows, columns, row_type, column_type, data):
        Table.__init__(self, html_class = 'image_table', rows = rows, columns = columns, \
                       row_type = row_type, column_type = column_type)

        self.images = []
        self.urls = []
        for i in range(len(rows)):
            d = []
            u = []
            for j in range(len(columns)):
                encoded_image = 'data:image/png;base64,\n'
                encoded_image += base64.b64encode(data[i][j][1])
                d.append(encoded_image)
                u.append(data[i][j][0])
            self.images.append(d)
            self.urls.append(u)

class Histograms(object):
    def __init__(self, parameters, observables, parameter_ranges, observable_ranges, N_bins = 25):
        self.parameter_parameter = []
        for parameter_y in parameters:
            row = []
            if parameter_y in parameter_ranges:
                lowy = parameter_ranges[parameter_y]['min']
                highy = parameter_ranges[parameter_y]['max']
                for parameter_x in parameters:
                    if parameter_x not in parameter_ranges:
                        row.append(None)
                        continue
                    lowx = parameter_ranges[parameter_x]['min']
                    highx = parameter_ranges[parameter_x]['max']
                    if parameter_x == parameter_y:
                        row.append(Histogram(N_bins, lowx, highx))
                    else:
                        row.append(Histogram2D(N_bins, lowx, highx, N_bins, lowy, highy))
            else:
                row = [None for p in parameters]
            self.parameter_parameter.append(row)

        self.observable_parameter = []
        self.observable_observable = []
        for observable_y in observables:
            parameter_row = []
            observable_row = []
            if observable_y in observable_ranges:
                lowy = observable_ranges[observable_y]['min']
                highy = observable_ranges[observable_y]['max']
                for parameter_x in parameters:
                    if parameter_x not in parameter_ranges:
                        parameter_row.append(None)
                        continue
                    lowx = parameter_ranges[parameter_x]['min']
                    highx = parameter_ranges[parameter_x]['max']
                    parameter_row.append(Histogram2D(N_bins, lowx, highx, N_bins, lowy, highy))
                for observable_x in observables:
                    if observable_x not in observable_ranges:
                        observable_row.append(None)
                        continue
                    lowx = observable_ranges[observable_x]['min']
                    highx = observable_ranges[observable_x]['max']
                    if observable_x == observable_y:
                        observable_row.append(Histogram(N_bins, lowx, highx))
                    else:
                        observable_row.append(Histogram2D(N_bins, lowx, highx, N_bins, lowy, highy))
            else:
                parameter_row = [None for p in parameters]
                observable_row = [None for p in observable]
            self.observable_parameter.append(parameter_row)
            self.observable_observable.append(observable_row)

        self.observables = observables
        self.parameters = parameters
        self.observable_ranges = observable_ranges

    def add_sample(self, parameters, observables):
        for i in range(len(observables)):
            for j in range(len(observables)):
                if not self.observable_observable[i][j]:
                    continue
                if i == j:
                    self.observable_observable[i][j].fill(observables[i])
                else:
                    self.observable_observable[i][j].fill(observables[j], observables[i])
            for j in range(len(parameters)):
                if not self.observable_parameter[i][j]:
                    continue
                self.observable_parameter[i][j].fill(parameters[j], observables[i])
        for i in range(len(parameters)):
            for j in range(len(parameters)):
                if not self.parameter_parameter[i][j]:
                    continue
                if i == j:
                    self.parameter_parameter[i][j].fill(parameters[i])
                else:
                    self.parameter_parameter[i][j].fill(parameters[j], parameters[i])

class Correlations(object):
    def __init__(self, number_of_parameters, number_of_observables):
        self.number_of_parameters = number_of_parameters
        self.number_of_observables = number_of_observables
        self.samples = 0
        self.po_squares = [ [0.0]*number_of_observables for i in range(number_of_parameters) ]
        self.pp_squares = [ [0.0]*number_of_parameters for i in range(number_of_parameters) ]
        self.parameter_sums = [0.0]*number_of_parameters
        self.observable_sums = [0.0]*number_of_observables
        self.parameter_squares = [0.0]*number_of_parameters
        self.observable_squares = [0.0]*number_of_observables
        self.loglikelihood_sums = [0.0]*number_of_observables

        # De represents the derivative with respect to the error on the observables
        self.De_samples = [0.0]*number_of_observables
        self.De_loglikelihood_sums = [0.0]*number_of_observables
        self.De_parameter_sums = []
        self.De_pp_squares = []
        for i in range(number_of_observables):
            self.De_parameter_sums.append(copy.deepcopy(self.parameter_sums))
            self.De_pp_squares.append(copy.deepcopy(self.pp_squares))

        # Dv represents the derivative with respect to the value of the observable
        self.Dv_samples = [0.0]*number_of_observables
        self.Dv_loglikelihood_sums = [0.0]*number_of_observables
        self.Dv_parameter_sums = []
        self.Dv_pp_squares = []
        for i in range(number_of_observables):
            self.Dv_parameter_sums.append(copy.deepcopy(self.parameter_sums))
            self.Dv_pp_squares.append(copy.deepcopy(self.pp_squares))


    def add_sample(self, parameters, observables, loglikelihood, value_gradient, error_gradient):
        self.samples += 1
        for i, parameter in enumerate(parameters):
            self.parameter_sums[i] += parameter
            self.parameter_squares[i] += parameter**2
            for j in range(i+1):
                self.pp_squares[i][j] += parameter*parameters[j]
            for j, observable in enumerate(observables):
                self.po_squares[i][j] += parameter*observable
        for k, observable in enumerate(observables):
            self.observable_sums[k] += observable
            self.observable_squares[k] += observable**2
            self.Dv_samples[k] += value_gradient[k]
            self.De_samples[k] += error_gradient[k]
            self.Dv_loglikelihood_sums[k] += value_gradient[k]*loglikelihood
            self.De_loglikelihood_sums[k] += error_gradient[k]*loglikelihood
            self.loglikelihood_sums[k] += loglikelihood
            for i, parameter in enumerate(parameters):
                self.Dv_parameter_sums[k][i] += parameter*value_gradient[k]
                self.De_parameter_sums[k][i] += parameter*error_gradient[k]
                for j in range(i+1):
                    self.Dv_pp_squares[k][i][j] += parameter*parameters[j]*value_gradient[k]
                    self.De_pp_squares[k][i][j] += parameter*parameters[j]*error_gradient[k]

    def parameter_mean(self, i):
        return self.parameter_sums[i]/float(self.samples)

    def parameter_standard_deviation(self, i):
        x2 = self.parameter_squares[i]/float(self.samples)
        x = self.parameter_mean(i)
        return x2 - x*x

    def observable_mean(self, i):
        return self.observable_sums[i]/float(self.samples)

    def observable_standard_deviation(self, i):
        y2 = self.observable_squares[i]/float(self.samples)
        y = self.observable_mean(i)
        return y2 - y*y

    def parameter_parameter_covariance(self, i, j):
        if j > i:
            i, j = j, i
        N = float(self.samples)
        return (self.pp_squares[i][j] - (self.parameter_sums[i]*self.parameter_sums[j]/N))/N

    def parameter_variance(self, i):
        return self.parameter_parameter_covariance(i, i)

    def parameter_observable_covariance(self, i, j):
        N = float(self.samples)
        return (self.po_squares[i][j] - (self.parameter_sums[i]*self.observable_sums[j]/N))/N


    def De_parameter_parameter_covariance(self, k, i, j):
        N = float(self.samples)
        De_covariance = self.De_pp_squares[k][i][j]/N
        De_covariance -= self.pp_squares[i][j]*self.De_samples[k]/(N*N)

        De_covariance -= self.parameter_mean(i)*self.De_parameter_sums[k][j]/N
        De_covariance += self.parameter_mean(i)*self.parameter_mean(j)*self.De_samples[k]/N

        De_covariance -= self.parameter_mean(j)*self.De_parameter_sums[k][i]/N
        De_covariance += self.parameter_mean(j)*self.parameter_mean(i)*self.De_samples[k]/N

        return De_covariance/self.parameter_parameter_covariance(i, j)
    
    def De_parameter_parameter_root_covariance(self, k, i, j):
        return self.De_parameter_parameter_covariance(k, i, j)/2.0

    def De_parameter_standard_deviation(self, k, i):
        return self.De_parameter_parameter_root_covariance(k, i, i)

    def parameter_response_to_value(self, observable_index, parameter_index):
        Dv_parameter_mean = self.Dv_parameter_sums[observable_index][parameter_index]/float(self.samples)
        parameter_mean = self.parameter_mean(parameter_index)
        Dv_mean = self.Dv_samples[observable_index]/float(self.samples)

        return Dv_parameter_mean - parameter_mean*Dv_mean

    def parameter_response_to_error(self, observable_index, parameter_index):
        De_parameter_mean = self.De_parameter_sums[observable_index][parameter_index]/float(self.samples)
        parameter_mean = self.parameter_mean(parameter_index)
        De_mean = self.De_samples[observable_index]/float(self.samples)

        return De_parameter_mean - parameter_mean*De_mean

    def loglikelihood_response_to_value(self, observable_index):
        Dv_loglikelihood_mean = self.Dv_loglikelihood_sums[observable_index]/float(self.samples)
        loglikelihood_mean = self.loglikelihood_sums[observable_index]/float(self.samples)
        Dv_mean = self.Dv_samples[observable_index]/float(self.samples)

        return Dv_loglikelihood_mean - loglikelihood_mean*Dv_mean

    def resolving_power(self, k , i, j):
        if i == j:
            return self.De_parameter_standard_deviation(k, i)

        variance = (self.parameter_variance(i), self.parameter_variance(j))
        standard_deviation = (math.sqrt(variance[0]), math.sqrt(variance[1]))
        covariance = self.parameter_parameter_covariance(i, j)

        # there's an arbitrary scale if the two parameters have different units
        # here we scale each by their standard deviation
        scale = (standard_deviation[0], standard_deviation[1])
        standard_deviation = (standard_deviation[0]/scale[0], standard_deviation[1]/scale[1])
        variance = (standard_deviation[0]**2, standard_deviation[1]**2)
        covariance /= scale[0]*scale[1]

        # pick pi/4 if they're the same, as is the case when scaled by the standard deviation
        major_theta = math.pi/4.0
        if variance[0] != variance[1]:
            major_theta = -0.5*math.atan(2.0*covariance/(variance[1]-variance[0]))
        minor_theta = major_theta + math.pi/2.0
        major_cos, major_sin = math.cos(major_theta), math.sin(major_theta)
        minor_cos, minor_sin = math.cos(minor_theta), math.sin(minor_theta)
        
        major_variance = (major_cos**2)*variance[0]
        major_variance += (major_sin**2)*variance[1]
        major_variance += 2.0*major_cos*major_sin*covariance

        minor_variance = (minor_cos**2)*variance[0]
        minor_variance += (minor_sin**2)*variance[1]
        minor_variance += 2.0*minor_cos*minor_sin*covariance

        #It's arbitrary which is larger so swap them if necessary
        if minor_variance > major_variance:
            minor_variance, major_variance = major_variance, minor_variance
            minor_theta, major_theta = major_theta, minor_theta
            minor_cos, major_cos = major_cos, minor_cos
            minor_sin, major_sin = major_sin, minor_sin
        major_standard_deviation = math.sqrt(major_variance)
        minor_standard_deviation = math.sqrt(minor_variance)

        De_standard_deviation = (self.De_parameter_standard_deviation(k, i), self.De_parameter_standard_deviation(k, j))

        De_major_variance = (major_cos**2)*2.0*variance[0]*De_standard_deviation[0]
        De_major_variance += (major_sin**2)*2.0*variance[1]*De_standard_deviation[1]
        De_major_variance += 2.0*major_cos*major_sin*covariance*self.De_parameter_parameter_covariance(k, i, j)

        De_minor_variance = (minor_cos**2)*2.0*variance[0]*De_standard_deviation[0]
        De_minor_variance += (minor_sin**2)*2.0*variance[1]*De_standard_deviation[1]
        De_minor_variance += 2.0*minor_cos*minor_sin*covariance*self.De_parameter_parameter_covariance(k, i, j)

        De_major_resolving_power = De_major_variance/(2.0*major_variance)
        De_minor_resolving_power = De_minor_variance/(2.0*minor_variance)

        if i < j:
            return De_major_resolving_power
        else:
            return De_minor_resolving_power

class TraceFileException(Exception):
    pass

def read_trace_file(filename, stats_dir = '.'):
    if not os.path.isfile(filename):
        raise TraceFileException('Trace file ' + filename + ' does not exist.')
    f = open(filename, 'rb')
    if f.read(1) == b'"':
        f.close()
        f = open(filename,'r')
    else:
        f.close()
        f = gzip.open(filename,'r')
    header = [name[1:-1] for name in f.readline()[:-1].split(',')]
    if header[-1] == 'LogLikelihood':
        raise TraceFileException('There are no log likelihood gradients in the trace file.')
    if 'LogLikelihood' not in header:
        raise TraceFileException('There is no log likelihood in the trace file.')
    loglikelihood_index = header.index('LogLikelihood')
    number_of_observables = 0
    for title in header[loglikelihood_index+1:]:
        if '*dLL/dsigma_' in title:
            break
        number_of_observables += 1
    number_of_parameters = loglikelihood_index - number_of_observables
    parameter_names = header[:number_of_parameters]
    observable_names = header[number_of_parameters:loglikelihood_index]
    parameter_ranges = parse_parameters(stats_dir)
    observable_ranges = parse_observables(stats_dir)
    correlations = Correlations(number_of_parameters, number_of_observables)
    histograms = Histograms(parameter_names, observable_names, parameter_ranges, observable_ranges)

    for line in f:
        line = map(float, line.split(','))
        parameters = line[:number_of_parameters]
        observables = line[number_of_parameters:loglikelihood_index]
        loglikelihood = line[loglikelihood_index]
        value_gradient = line[loglikelihood_index+1:loglikelihood_index+1+number_of_observables]
        error_gradient = line[loglikelihood_index+1+number_of_observables:loglikelihood_index+1+2*number_of_observables]
        correlations.add_sample(parameters, observables, loglikelihood, value_gradient, error_gradient)
        histograms.add_sample(parameters, observables)

    return (correlations, histograms, observable_names, parameter_names)

def build_data_tables(correlations, observable_names, parameter_names):
    tables = {}

    #resolving power
    resolving_power = []
    for i, observable in enumerate(observable_names):
        row = []
        for j, parameter in enumerate(parameter_names):
            row.append(correlations.resolving_power(i, j, j))
        resolving_power.append(row)
    tables['resolving_power'] = DataTable(rows = observable_names, columns = parameter_names, \
                                          row_type = 'observable', column_type = 'parameter', \
                                          data = resolving_power)

    #2d resolving power
    tables['resolving_power_2d'] = []
    for k, observable in enumerate(observable_names):
        resolving_power_2d = []
        for i, parameter in enumerate(parameter_names):
            row = []
            for j in range(len(parameter_names)):
                row.append(correlations.resolving_power(k, i, j))
            resolving_power_2d.append(row)
        table = DataTable(rows = parameter_names, columns = parameter_names, \
                          row_type = 'parameter', column_type = 'parameter', \
                          data = resolving_power_2d)
        tables['resolving_power_2d'].append(table)

    #parameter response
    parameter_response = []
    for k, observable in enumerate(observable_names):
        row = []
        for i, parameter in enumerate(parameter_names):
            row.append(correlations.parameter_response_to_value(k, i))
        parameter_response.append(row)
    tables['parameter_response'] = DataTable(rows = observable_names, columns = parameter_names, \
                                             row_type = 'observable', column_type = 'parameter', \
                                             data = parameter_response)
    tables['parameter_response'].white_out()

    #scaled parameter response
    scaled_parameter_response = []
    for k, observable in enumerate(observable_names):
        row = []
        for i, parameter in enumerate(parameter_names):
            row.append(parameter_response[k][i])
            row[i] *= correlations.observable_standard_deviation(k)
        scaled_parameter_response.append(row)
    tables['scaled_parameter_response'] = DataTable(rows = observable_names, columns = parameter_names, \
                                             row_type = 'observable', column_type = 'parameter', \
                                             data = scaled_parameter_response)
    tables['scaled_parameter_response'].colorize_by_column()

    #parameter response to error
    parameter_response_to_error = []
    for k, observable in enumerate(observable_names):
        row = []
        for i, parameter in enumerate(parameter_names):
            row.append(correlations.parameter_response_to_error(k, i))
        parameter_response_to_error.append(row)
    tables['parameter_response_to_error'] = DataTable(rows = observable_names, columns = parameter_names, \
                                                      row_type = 'observable', column_type = 'parameter', \
                                                      data = parameter_response_to_error)
    tables['parameter_response_to_error'].colorize_by_column()

    #loglikelihood response
    loglikelihood_response = []
    for k, observable in enumerate(observable_names):
        loglikelihood_response.append(correlations.loglikelihood_response_to_value(k))
    tables['loglikelihood_response'] = DataTable(rows = ['dLL/d_observable'], columns = observable_names, \
                                                      row_type = 'other', column_type = 'observable', \
                                                      data = [loglikelihood_response])

    #scaled loglikelihood response
    scaled_loglikelihood_response = []
    for i in range(len(loglikelihood_response)):
        scaled_loglikelihood_response.append(loglikelihood_response[i])
        scaled_loglikelihood_response[i] *= correlations.observable_standard_deviation(i)
    tables['scaled_loglikelihood_response'] = DataTable(rows = ['sigma_observable*dLL/d_observable'], columns = observable_names, \
                                                      row_type = 'other', column_type = 'observable', \
                                                      data = [scaled_loglikelihood_response])
    return tables

def build_image_tables(histograms, outdir=None):
    tables = {}

    #try to make the png files small enough...
    observable_width = float(PIXEL_WIDTH/float(len(histograms.observables)))
    parameter_width = float(PIXEL_WIDTH/float(len(histograms.parameters)))

    observable_observable_plots = []
    observable_parameter_plots = []
    for i in range(len(histograms.observables)):
        observable_row, parameter_row = [], []
        for j in range(len(histograms.observables)):
            if not histograms.observable_observable[i][j]:
                observable_row.append('')
                continue
            xlabel, ylabel = histograms.observables[j], histograms.observables[i]
            if i == j:
                ylabel = 'counts'
                mu = histograms.observable_ranges[histograms.observables[i]]['mu']
                sigma = histograms.observable_ranges[histograms.observables[i]]['sigma']
                filename = None
                if outdir != None:
                    filename = histograms.observables[j] + '.svg'
                observable_row.append((filename, histograms.observable_observable[i][j].img(xlabel = xlabel, ylabel = ylabel, \
                                                                                 mu = mu, sigma = sigma, filename=outdir+filename, width=observable_width)))
                continue
            filename = None
            if outdir != None:
                filename = histograms.observables[i] + '_vs_' + histograms.observables[j] + '.svg'
            observable_row.append((filename, histograms.observable_observable[i][j].img(xlabel = xlabel, ylabel = ylabel, filename=outdir+filename, width=observable_width)))
        for j in range(len(parameters)):
            if not histograms.observable_parameter[i][j]:
                parameter_row.append('')
                continue
            xlabel, ylabel = histograms.parameters[j], histograms.observables[i]
            filename = None
            if outdir != None:
                filename = histograms.observables[i] + '_vs_' + histograms.parameters[j] + '.svg'
            parameter_row.append((filename, histograms.observable_parameter[i][j].img(xlabel = xlabel, ylabel = ylabel, filename=outdir+filename, width=parameter_width)))
        observable_observable_plots.append(observable_row)
        observable_parameter_plots.append(parameter_row)
    tables['observable_observable_plots'] = ImageTable(rows = histograms.observables, columns = histograms.observables, \
                                                       row_type = 'observable', column_type = 'observable', \
                                                       data = observable_observable_plots)
    tables['observable_parameter_plots'] = ImageTable(rows = histograms.observables, columns = histograms.parameters, \
                                                      row_type = 'observable', column_type = 'parameter', \
                                                      data = observable_parameter_plots)

    parameter_parameter_plots = []
    for i in range(len(histograms.parameters)):
        parameter_row = []
        for j in range(len(histograms.parameters)):
            if not histograms.parameter_parameter[i][j]:
                parameter_row.append('')
                continue
            if i == j:
                ylabel = 'counts'
            xlabel, ylabel = histograms.parameters[j], histograms.parameters[i]
            filename = None
            if outdir != None:
                filename = histograms.parameters[i] + '_vs_' + histograms.parameters[j] + '.svg'
                if i == j:
                    filename = histograms.parameters[i] + '.svg'
            parameter_row.append((filename, histograms.parameter_parameter[i][j].img(xlabel = xlabel, ylabel = ylabel, filename=outdir+filename, width=parameter_width)))
        parameter_parameter_plots.append(parameter_row)
    tables['parameter_parameter_plots'] = ImageTable(rows = histograms.parameters, columns = histograms.parameters, \
                                                     row_type = 'parameter', column_type = 'parameter', \
                                                     data = parameter_parameter_plots)

    return tables

def parse_parameters(stats_dir):
    parameters = {}
    with open(stats_dir + '/parameter_priors.dat', 'r') as f:
        for line in f:
            distribution, name, vmin, vmax = line.split()
            vmin, vmax = float(vmin), float(vmax)
            distribution = distribution.lower()
            if distribution != 'uniform':
                mu, sigma = vmin, vmax
                vmin, vmax = mu - sigma*N_SIGMA, mu + sigma*N_SIGMA
            parameters[name] = {'min' : vmin, 'max' : vmax}
    return parameters

def parse_observables(stats_dir):
    observables = {}
    with open(stats_dir + '/experimental_results.dat', 'r') as f:
        for line in f:
            name, mu, sigma = line.split()
            mu, sigma = float(mu), float(sigma)
            vmin, vmax = mu - sigma*N_SIGMA, mu + sigma*N_SIGMA
            observables[name] = {'min' : vmin, 'max' : vmax, 'mu' : mu, 'sigma' : sigma}
    return observables

if __name__ == '__main__':
    if len(sys.argv) != 4:
        print 'Usage: ' + sys.argv[0] + '/stats/directory/ trace_file.csv web_directory'
        exit()
    try:
        outdir = sys.argv[3]
        if outdir[-1] != '/':
            outdir += '/'
        if not os.path.exists(outdir):
            os.makedirs(outdir)

        correlations, histograms, observables, parameters = read_trace_file(sys.argv[2], sys.argv[1])
        tables = build_data_tables(correlations, observables, parameters)
        tables['observables'] = observables
        tables['parameters'] = parameters
        for key, val in build_image_tables(histograms, outdir).iteritems():
            tables[key] = val

        from jinja2 import Template
        template_path = os.path.dirname(os.path.realpath(__file__)) + '/overview_template.html'
        with open(template_path, 'r') as template_file:
            with open(outdir + 'index.html', 'w') as output_file:
                template = Template(template_file.read())
                output_file.write(template.render(**tables))

    except TraceFileException as e:
        print 'Fatal exception: ', e
